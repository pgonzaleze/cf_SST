{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning algorithms for coral bleaching classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Import libraries\n",
    "'''\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import sklearn\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean \n",
    "from numpy import std\n",
    "import pingouin as pg   \n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import levene \n",
    "from scipy.stats import bartlett\n",
    "from scipy.stats import kruskal\n",
    "import scikit_posthocs as sp \n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from itertools import combinations, permutations\n",
    "# check scikit-learn version\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Load full dataset\n",
    "'''\n",
    "data = pd.read_csv('df_sst_clouds.csv',low_memory=False)\n",
    "len(data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Subset DF by SEVERITY_CODE [0,1,2,3]\n",
    "      Severity_code == \"-1\" is dropped\n",
    "'''\n",
    "#data = data.dropna() # drop rows that contains NaN's \n",
    "data = data[(data.SEVERITY_CODE == 0)|(data.SEVERITY_CODE == 1)|(data.SEVERITY_CODE == 2)|(data.SEVERITY_CODE == 3)] \n",
    "#data = data[(data.YEAR >= 2002)] # First year with more than 100 records\n",
    "#data = data[(data.YEAR >= 2015) & (data.YEAR <= 2016)] # subset a single event\n",
    "#data = data[(data.YEAR >= 1997) & (data.YEAR <= 1998)]\n",
    "#list(data.columns)\n",
    "data = data.dropna() # drop rows that contains NaN's (if any)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "            Restricting to cells with an actual DHW value only\n",
    "    This step should be used after getting results of complegte dataset\n",
    "'''\n",
    "# data = data.drop(data[(data.DHW == 0) & (data.DHW_RMax30 == 0) & (data.DHW_RMax60 == 0) &(data.DHW_RMax90 == 0) & (data.SEVERITY_CODE > 0)].index) \n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "        Create dataframes to build independent models \n",
    "        Example:\n",
    "        X1 = data[['var #1','var #2']]  ## features (dependen variable(s))\n",
    "        X2 = data[['var #1','var #3']] \n",
    "        y = data[['independent variable']]  ## labels (indipendent variable)\n",
    "        The object \"X_\" can be used to run a specific model, however, to keep it simple, the object \"X\" is going to be used and edited each time to get printed the results and avoid changing the code in subsequent steps\" \n",
    "'''\n",
    "# ## Features (dependent variable)\n",
    "# X0=data[['DHW']] # better than any othe DHW metric\n",
    "# X00=data[['DHW_9']] # better than DHW\n",
    "# X1=data[['DHW', 'CF']] \n",
    "# X2=data[['DHW', 'CFrunmean7']]\n",
    "# X3=data[['DHW', 'CFrunmean30']] \n",
    "# X4=data[['DHW', 'CFrunmean90']] # \n",
    "# X5=data[['DHW', 'CF_a']]\n",
    "# X6=data[['DHW', 'CF_a_runmean7']] #\n",
    "# X7=data[['DHW', 'CF_a_runmean30']] #\n",
    "# X8=data[['DHW', 'CF_a_runmean90']] # +\n",
    "# X9=data[['DHW_9','CF_a']] # + better than DHW_9\n",
    "# X10=data[['DHW_9','CFrunmean7']]\n",
    "# X11=data[['DHW_9','CFrunmean30']]\n",
    "# X12=data[['DHW_9','CFrunmean90']]\n",
    "# X13=data[['DHW_9','CF_a_runmean7']] # \n",
    "# X12=data[['DHW_9','CF_a_runmean7']] # +\n",
    "# X15=data[['DHW_9','CF_a_runmean90']] # \n",
    "# X16=data[['DHW_9','CFrunmean7','CD']] # x\n",
    "# X17=data[['DHW_9','CFrunmean7','WD']] # x\n",
    "# X18=data[['SSTrunmax90','CF_a_runmean90','WD']] # +++\n",
    "# X19=data[['DHW','CF_a_runmean7','CV_run7']] # +\n",
    "# X20=data[['DHW','CF_a_runmean30','CV_run30']] # +\n",
    "# X21=data[['DHW','CF_a_runmean90','CV_run90']] # -\n",
    "# X22=data[['SSTrunmean90','CF_a_runmean90','WD']] # +\n",
    "# X23=data[['SSTrunmean7','CF_a_runmean90','WD']]\n",
    "# X24=data[['SSTrunmax7','CF_a_runmean90','WD']] # ++\n",
    "# X25=data[['YEAR','DHW_9','CF_a_runmean7']] # +\n",
    "# X26=data[['YEAR','DHW','CF_a_runmean90']] # +\n",
    "# X27=data[['YEAR','CF_a_runmean90']] # -   VIF + 1,5\n",
    "# X28=data[['YEAR','SSTrunmax7','CFrunmean90']] # \n",
    "# X30=data[['YEAR','SSTrunmax90','DHW','CV_run90']] # +++\n",
    "# X31=data[['YEAR','SSTrunmax90','DHW','CV_run90','CFrunmean90']] # +++\n",
    "# X32=data[['DHW_adj_date', 'CFrunmean90_adj_date']] # YEAR + DHW_9 VIF + 1.5\n",
    "# #labels (indipendent variable)\n",
    "# y=data['SEVERITY_CODE'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:,('DHW','CF_a_runmean90')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Iterable object to run the models \n",
    "Here you can set the model's desired variables\n",
    "'''\n",
    "X=data.loc[:,('DHW','CF_a_runmean30')] ## Features (dependent variable(s)); select desired variables\n",
    "y=data['SEVERITY_CODE'] # labels (indipendent variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Variance inflation factor VIF\n",
    "'''\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Get variables for which to compute VIF and add intercept term\n",
    "X['Intercept'] = 1\n",
    "# Compute and view VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variables\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "# View results using print\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Build the models\n",
    "'''\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=3)\n",
    "model.fit(X,y)\n",
    "# evaluate the model\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=3)\n",
    "n_scores = cross_val_score(model, X, y, cv=cv) #n_jobs=-1, error_score='raise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Report performance\n",
    "'''\n",
    "print('Cross val score: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "print('\\n')\n",
    "# Features importance\n",
    "print('=== features importances ===')\n",
    "fi = pd.DataFrame({'feature': list(X.columns),\n",
    "                   'importance': model.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Confusion matrix\n",
    "'''\n",
    "y_pred=model.predict(X)\n",
    "conf_mat = confusion_matrix(y, y_pred)\n",
    "conf_mat_norm = confusion_matrix(y, y_pred,normalize='all')\n",
    "print(\"=== Confusion matrix ===\")\n",
    "print(conf_mat)\n",
    "print('\\n')\n",
    "print(\"=== Confusion matrix normalized ===\")\n",
    "print(conf_mat_norm)\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y, y_pred))\n",
    "print('\\n')\n",
    "print('=== Accuracy and Kappa ===')\n",
    "print('accuracy', metrics.accuracy_score(y, y_pred))\n",
    "print('\\n')\n",
    "print('kappa', metrics.cohen_kappa_score(y, y_pred))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Evaluation between classifications models through \"log loss\"\n",
    "'''\n",
    "model_probs = model.predict_proba(X)\n",
    "score = log_loss(y, model_probs)\n",
    "\n",
    "'''\n",
    "    Evaluation between classifications models through \"ROC_AUC\"\n",
    "'''\n",
    "roc_value = roc_auc_score(y, model_probs, multi_class='ovo') # ovo': Computes the average AUC of all possible pairwise combinations of classes\n",
    "\n",
    "\n",
    "print('=== roc_auc_score ===') \n",
    "print(roc_value)\n",
    "print(' ')\n",
    "print('=== log_loss_score ===') \n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "ax = plt.subplot()\n",
    "#Heat map with annot=True to annotate cells\n",
    "sns.heatmap(conf_mat, annot=True, ax = ax, fmt='d', cmap='Blues') # actual cases\n",
    "#sns.heatmap(conf_mat/np.sum(conf_mat), annot=True, ax = ax, fmt='.2%', cmap='Blues') #percentage\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted bleaching level');ax.set_ylabel('True bleaching level'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['level 0','level 1','level 2', 'level 3']); ax.yaxis.set_ticklabels(['level 0','level 1','level 2', 'level 3'])\n",
    "#plt.show()\n",
    "#plt.savefig('DHW9_CF_a_runmean90.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = pd.DataFrame({'Real':y, 'Predictions':y_pred})\n",
    "comparisons.to_csv('pred_DHW_CF_a_runeman90.csv')\n",
    "print(comparisons[['Real','Predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('df_for_RFoutputs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('bmped': virtualenv)",
   "language": "python",
   "name": "python37464bitbmpedvirtualenv246c9788028a4da1852168913cf90212"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}